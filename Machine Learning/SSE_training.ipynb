{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9dd207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data (replace with your data) ---> actual values\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array([1, 2.8, 3.6, 4.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df121760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "theta_0 = 0   # b\n",
    "theta_1 = 0   # w\n",
    "alpha = 0.01  # learning rate\n",
    "num_iterations = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfddba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the SSE values for plotting:\n",
    "sse_values = []  # Sum Squared Error between (actual values and predicted values) = Loss\n",
    "\n",
    "# Gradient descent:\n",
    "'''Gradient Descent is an iterative algorithm used to minimize a loss function (like SSE) \n",
    "   by adjusting the model's parameters (weights and biases) step by step.'''\n",
    "for i in range(num_iterations):\n",
    "    # compute predictions\n",
    "    y_pred = theta_0 + theta_1 * x\n",
    "\n",
    "    # compute gradients\n",
    "    d_theta_0 = 2 * np.sum(y_pred - y)       # b\n",
    "    d_theta_1 = 2 * np.sum((y_pred - y) * x) # w\n",
    "\n",
    "    # update parameters\n",
    "    theta_0 -= alpha * d_theta_0\n",
    "    theta_1 -= alpha * d_theta_1\n",
    "\n",
    "    # compute SSE and store it\n",
    "    sse = np.sum((y - y_pred) ** 2)\n",
    "    sse_values.append(sse)\n",
    "\n",
    "    # Debug: print the SSE every 100 iterations\n",
    "    if(i+1) % 100 == 0:\n",
    "        print(f\"Iteration {i+1}, SSE: {sse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the SSE values to show convergence:\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# SSE over iterations:\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(num_iterations), sse_values, label='SSE')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('SSE')\n",
    "plt.title('SSE over Iterations')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the data points and the final regression line:\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x, y, color='blue', label='Data Points')\n",
    "plt.plot(x, theta_0 + theta_1 * x, color='red', label='Regression Line')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Fit')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimized parameters: theta_0 (bias) = {theta_0}, theta_1 (weight) = {theta_1}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
